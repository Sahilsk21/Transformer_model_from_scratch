{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"V28","mount_file_id":"1mIPIfy0Bc8ELTh8uiKgw7BF1uD4RocLL","authorship_tag":"ABX9TyNUgmn0s4p1IJKiDUVRv2qJ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"TPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"d-buo8KtXyQ8","executionInfo":{"status":"ok","timestamp":1721392864513,"user_tz":-330,"elapsed":4852,"user":{"displayName":"Sahil Sk","userId":"12371162085645398510"}}},"outputs":[],"source":["## import\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import tensorflow as tf\n","from tensorflow import convert_to_tensor, string\n","from tensorflow.keras.layers import TextVectorization, Embedding, Layer,Dense\n","from tensorflow.data import Dataset\n","from tensorflow.keras import Model,Input"]},{"cell_type":"code","source":["output_sequence_length = 5\n","vocab_size = 10\n","sentences = [[\"I am a robot\"], [\"you too robot\"]]\n","sentence_data = Dataset.from_tensor_slices(sentences)"],"metadata":{"id":"jvZz2ae1YWrX","executionInfo":{"status":"ok","timestamp":1721392864514,"user_tz":-330,"elapsed":23,"user":{"displayName":"Sahil Sk","userId":"12371162085645398510"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["sentence_data"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TUTrMJ8NZG1K","executionInfo":{"status":"ok","timestamp":1721392864514,"user_tz":-330,"elapsed":23,"user":{"displayName":"Sahil Sk","userId":"12371162085645398510"}},"outputId":"4d79e2c7-1e68-4a63-bdfc-91ad18d3ce2c"},"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<_TensorSliceDataset element_spec=TensorSpec(shape=(1,), dtype=tf.string, name=None)>"]},"metadata":{},"execution_count":3}]},{"cell_type":"code","source":["# Create the TextVectorization layer\n","vectorize_layer=TextVectorization(max_tokens=vocab_size,output_sequence_length=output_sequence_length)\n","# Train the layer to create a dictionary\n","vectorize_layer.adapt(sentence_data)\n","# Convert all sentences to tensors\n","word_tensors = convert_to_tensor(sentences, dtype=tf.string)"],"metadata":{"id":"k7P2uM0FZIvy","executionInfo":{"status":"ok","timestamp":1721392864515,"user_tz":-330,"elapsed":22,"user":{"displayName":"Sahil Sk","userId":"12371162085645398510"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["# Use the word tensors to get vectorized phrases\n","vectorized_words = vectorize_layer(word_tensors)\n","print(\"Vocabulary: \", vectorize_layer.get_vocabulary())\n","print(\"Vectorized words: \", vectorized_words)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Bdv0U3HiZs5l","executionInfo":{"status":"ok","timestamp":1721392864515,"user_tz":-330,"elapsed":22,"user":{"displayName":"Sahil Sk","userId":"12371162085645398510"}},"outputId":"2c0dfe31-bc10-4830-e3d2-8bff44f43d3f"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Vocabulary:  ['', '[UNK]', 'robot', 'you', 'too', 'i', 'am', 'a']\n","Vectorized words:  tf.Tensor(\n","[[5 6 7 2 0]\n"," [3 4 2 0 0]], shape=(2, 5), dtype=int64)\n"]}]},{"cell_type":"code","source":["## embedding layer\n","output_length = 6\n","word_embedding_layer=Embedding(vocab_size,output_length)\n","embedded_words=word_embedding_layer(vectorized_words)\n","print(embedded_words)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"15sVPMcDZuiS","executionInfo":{"status":"ok","timestamp":1721392864515,"user_tz":-330,"elapsed":20,"user":{"displayName":"Sahil Sk","userId":"12371162085645398510"}},"outputId":"d082ffa9-df21-423c-e484-8fb3fe9c153d"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["tf.Tensor(\n","[[[ 0.01802111 -0.03655459  0.04699257 -0.03549253 -0.03372033\n","   -0.04348135]\n","  [-0.03627764 -0.00324202  0.03054514 -0.01772243 -0.00555078\n","   -0.03975026]\n","  [-0.04530586 -0.01655366  0.01745584  0.03302291  0.02491238\n","   -0.02427235]\n","  [ 0.04008334 -0.0498448   0.00774311  0.01157374  0.03944308\n","    0.03408824]\n","  [-0.0132429  -0.01852164 -0.00074568 -0.00759749 -0.02971439\n","   -0.04123703]]\n","\n"," [[ 0.02138865  0.0447805  -0.00412636  0.01468355 -0.04517774\n","    0.02160535]\n","  [-0.04765559  0.04301833  0.04237645 -0.04278549  0.00940941\n","    0.03539101]\n","  [ 0.04008334 -0.0498448   0.00774311  0.01157374  0.03944308\n","    0.03408824]\n","  [-0.0132429  -0.01852164 -0.00074568 -0.00759749 -0.02971439\n","   -0.04123703]\n","  [-0.0132429  -0.01852164 -0.00074568 -0.00759749 -0.02971439\n","   -0.04123703]]], shape=(2, 5, 6), dtype=float32)\n"]}]},{"cell_type":"code","source":["## postional embedding\n","position_embedding_layer=Embedding(output_sequence_length,output_length)\n","position_indices=tf.range(output_sequence_length)\n","position_embeddings=position_embedding_layer(position_indices)\n","print(position_embeddings)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9iLg5oJvbxRh","executionInfo":{"status":"ok","timestamp":1721392864515,"user_tz":-330,"elapsed":19,"user":{"displayName":"Sahil Sk","userId":"12371162085645398510"}},"outputId":"5d30fa45-6967-4abd-9153-96e653c56ca0"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["tf.Tensor(\n","[[ 0.03769987 -0.00326257  0.04162018  0.00956613  0.03597886 -0.03289197]\n"," [-0.02051154 -0.03148699 -0.03174626  0.00295677 -0.01347339  0.00110074]\n"," [-0.04847406  0.03707666  0.02330916  0.00303983 -0.02287695 -0.01452228]\n"," [-0.02172968  0.01392375 -0.04243938  0.02937079 -0.00286959  0.02439983]\n"," [ 0.02124126  0.01907993  0.04998076  0.01471615  0.01924132  0.01740256]], shape=(5, 6), dtype=float32)\n"]}]},{"cell_type":"code","source":["final_output_embedding=embedded_words+position_embeddings\n","print(final_output_embedding)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qeGczAjldU_J","executionInfo":{"status":"ok","timestamp":1721392864515,"user_tz":-330,"elapsed":16,"user":{"displayName":"Sahil Sk","userId":"12371162085645398510"}},"outputId":"f368ae69-8f29-4cb9-bb06-d219d62cdf37"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["tf.Tensor(\n","[[[ 0.05572098 -0.03981715  0.08861275 -0.0259264   0.00225853\n","   -0.07637332]\n","  [-0.05678918 -0.034729   -0.00120112 -0.01476566 -0.01902417\n","   -0.03864951]\n","  [-0.09377992  0.02052299  0.040765    0.03606273  0.00203543\n","   -0.03879462]\n","  [ 0.01835365 -0.03592105 -0.03469627  0.04094453  0.03657348\n","    0.05848807]\n","  [ 0.00799836  0.00055828  0.04923509  0.00711866 -0.01047307\n","   -0.02383448]]\n","\n"," [[ 0.05908852  0.04151793  0.03749382  0.02424968 -0.00919888\n","   -0.01128662]\n","  [-0.06816714  0.01153134  0.01063019 -0.03982872 -0.00406399\n","    0.03649175]\n","  [-0.00839072 -0.01276815  0.03105227  0.01461357  0.01656613\n","    0.01956597]\n","  [-0.03497259 -0.00459789 -0.04318506  0.02177329 -0.03258399\n","   -0.01683721]\n","  [ 0.00799836  0.00055828  0.04923509  0.00711866 -0.01047307\n","   -0.02383448]]], shape=(2, 5, 6), dtype=float32)\n"]}]},{"cell_type":"code","source":["## positional embedding for sub class\n","class PositionEmbeddingLayer(Layer):\n","  def __init__(self, sequence_length,vocab_size ,output_dim, **kwargs):\n","    super().__init__(**kwargs)\n","    self.wordembedding=Embedding(vocab_size,output_dim)\n","    self.positionembedding=Embedding(sequence_length,output_dim)\n","\n","  def call(self,inputs):\n","    position=tf.range(tf.shape(inputs)[-1])\n","    wmbadding_word=self.wordembedding(inputs)\n","    position_embedding=self.positionembedding(position)\n","    return wmbadding_word+position_embedding"],"metadata":{"id":"ak5GeaJHeCzF","executionInfo":{"status":"ok","timestamp":1721392864516,"user_tz":-330,"elapsed":14,"user":{"displayName":"Sahil Sk","userId":"12371162085645398510"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["my_embedding_layer = PositionEmbeddingLayer(output_sequence_length,\n","vocab_size, output_length)\n","embedded_layer_output = my_embedding_layer(vectorized_words)\n","print(\"Output from my_embedded_layer: \", embedded_layer_output)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8T-tzCoTf8Xi","executionInfo":{"status":"ok","timestamp":1721392864516,"user_tz":-330,"elapsed":14,"user":{"displayName":"Sahil Sk","userId":"12371162085645398510"}},"outputId":"3f5ffded-5ecc-499e-86ca-e1fdf9ca233b"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Output from my_embedded_layer:  tf.Tensor(\n","[[[-0.02115345  0.04130647  0.03703581  0.00608994  0.06194867\n","    0.07104433]\n","  [ 0.0491291   0.01616457 -0.04448112  0.08892301  0.01295964\n","    0.02418089]\n","  [-0.02373423 -0.05806943 -0.0478638   0.01348039 -0.01520481\n","   -0.04423137]\n","  [-0.00225726 -0.06576176 -0.0097317   0.06734931 -0.02840799\n","   -0.0123294 ]\n","  [-0.00975649  0.00876598  0.01595427  0.0734329   0.03291627\n","   -0.0702059 ]]\n","\n"," [[-0.00794388  0.047996    0.01193348 -0.05446617  0.00362823\n","    0.05957271]\n","  [ 0.04448646 -0.03431307 -0.00950992  0.0209864  -0.02023845\n","   -0.04165429]\n","  [-0.01525407 -0.06365989 -0.01925647  0.01484144 -0.02852146\n","   -0.04437854]\n","  [-0.03693442  0.0050489  -0.00548879  0.08491301  0.01095913\n","   -0.01831951]\n","  [-0.00975649  0.00876598  0.01595427  0.0734329   0.03291627\n","   -0.0702059 ]]], shape=(2, 5, 6), dtype=float32)\n"]}]},{"cell_type":"code","source":["class PositionEmbeddingFixedWeights(Layer):\n","  def __init__(self, sequence_length, vocab_size, output_dim, **kwargs):\n","    super().__init__(**kwargs)\n","    word_embedding_matrix = self.get_position_encoding(vocab_size, output_dim)\n","    pos_embedding_matrix = self.get_position_encoding(sequence_length, output_dim)\n","    self.word_embedding_layer = Embedding(\n","    input_dim=vocab_size, output_dim=output_dim,\n","    weights=[word_embedding_matrix],\n","    trainable=False\n","    )\n","    self.position_embedding_layer = Embedding(\n","    input_dim=sequence_length, output_dim=output_dim,\n","    weights=[pos_embedding_matrix],\n","    trainable=False\n","    )\n","\n","\n","\n","  def get_position_encoding(self, seq_len, d, n=10000):\n","    P=np.zeros((seq_len, d))\n","    for k in range(seq_len):\n","        for i in np.arange(int(d/2)):\n","            denominator = np.power(n, 2*i/d)\n","            P[k, 2*i] = np.sin(k/denominator)\n","            P[k, 2*i+1] = np.cos(k/denominator)\n","    return P\n","  def call(self, inputs):\n","    position_indices = tf.range(tf.shape(inputs)[-1])\n","    embedded_words = self.word_embedding_layer(inputs)\n","    embedded_indices = self.position_embedding_layer(position_indices)\n","    return embedded_words + embedded_indices"],"metadata":{"id":"7yMtKu5SgMx4","executionInfo":{"status":"ok","timestamp":1721392864516,"user_tz":-330,"elapsed":12,"user":{"displayName":"Sahil Sk","userId":"12371162085645398510"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["attnisallyouneed_embedding = PositionEmbeddingFixedWeights(output_sequence_length,\n","vocab_size, output_length)\n","attnisallyouneed_output = attnisallyouneed_embedding(vectorized_words)\n","print(\"Output from my_embedded_layer: \", attnisallyouneed_output)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6MQvGBlGjd6B","executionInfo":{"status":"ok","timestamp":1721392864516,"user_tz":-330,"elapsed":11,"user":{"displayName":"Sahil Sk","userId":"12371162085645398510"}},"outputId":"3e0ebaea-7250-4182-bddf-9b87b48855bc"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Output from my_embedded_layer:  tf.Tensor(\n","[[[-0.9589243   1.2836622   0.23000172  1.9731903   0.01077196\n","    1.9999421 ]\n","  [ 0.56205547  1.5004725   0.3213085   1.9603932   0.01508068\n","    1.9999142 ]\n","  [ 1.566284    0.3377554   0.41192317  1.9433732   0.01938933\n","    1.999877  ]\n","  [ 1.0504174  -1.4061394   0.2314966   1.9860148   0.01077211\n","    1.9999698 ]\n","  [-0.7568025   0.3463564   0.18459873  1.982814    0.00861763\n","    1.9999628 ]]\n","\n"," [[ 0.14112     0.0100075   0.1387981   1.9903207   0.00646326\n","    1.9999791 ]\n","  [ 0.08466846 -0.11334133  0.23099795  1.9817369   0.01077207\n","    1.9999605 ]\n","  [ 1.8185948  -0.8322937   0.185397    1.9913884   0.00861771\n","    1.9999814 ]\n","  [ 0.14112     0.0100075   0.1387981   1.9903207   0.00646326\n","    1.9999791 ]\n","  [-0.7568025   0.3463564   0.18459873  1.982814    0.00861763\n","    1.9999628 ]]], shape=(2, 5, 6), dtype=float32)\n"]}]},{"cell_type":"code","source":["## scale dot product attention\n","from tensorflow import matmul, math, cast, float32\n","from tensorflow.keras.layers import Layer\n","from keras.backend import softmax\n","# Implementing the Scaled-Dot Product Attention\n","class DotProductAttention(Layer):\n","    def __init__(self, **kwargs):\n","        super().__init__(**kwargs)\n","    def call(self, queries, keys, values, d_k, mask=None):\n","    # Scoring the queries against the keys after transposing the latter, and scaling\\\n","\n","        scores = matmul(queries, keys, transpose_b=True) / math.sqrt(cast(d_k, float32))\n","        # Apply mask to the attention scores\n","        if mask is not None:\n","            scores += -1e9 * mask\n","        # Computing the weights by a softmax operation\n","        weights = softmax(scores)\n","        # Computing the attention by a weighted sum of the value vectors\n","        return matmul(weights, values)"],"metadata":{"id":"eqTC_nhBjtJB","executionInfo":{"status":"ok","timestamp":1721392864516,"user_tz":-330,"elapsed":10,"user":{"displayName":"Sahil Sk","userId":"12371162085645398510"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["from numpy import random\n","input_seq_length = 5 # Maximum length of the input sequence\n","d_k = 64 # Dimensionality of the linearly projected queries and keys\n","d_v = 64 # Dimensionality of the linearly projected values\n","batch_size = 64 # Batch size from the training process\n","queries = random.random((batch_size, input_seq_length, d_k))\n","keys = random.random((batch_size, input_seq_length, d_k))\n","values = random.random((batch_size, input_seq_length, d_v))\n","attention = DotProductAttention()\n","print(attention(queries, keys, values, d_k))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Cl2l32yUp8rA","executionInfo":{"status":"ok","timestamp":1721392864516,"user_tz":-330,"elapsed":10,"user":{"displayName":"Sahil Sk","userId":"12371162085645398510"}},"outputId":"56ebc47c-343e-4b04-ccc9-79aa7208f32e"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["tf.Tensor(\n","[[[0.56642634 0.48043346 0.62086624 ... 0.49831706 0.6293844  0.47573417]\n","  [0.56624645 0.4665729  0.61928743 ... 0.49164605 0.6250012  0.4584907 ]\n","  [0.567589   0.48765612 0.626608   ... 0.49058715 0.6384107  0.48801014]\n","  [0.57554936 0.46686202 0.6245432  ... 0.4892829  0.62839735 0.46660364]\n","  [0.5682196  0.47088844 0.6196643  ... 0.4991469  0.6255311  0.46260226]]\n","\n"," [[0.33131826 0.29962105 0.4115417  ... 0.37526932 0.68500924 0.3527864 ]\n","  [0.3313189  0.30002838 0.41044855 ... 0.37486857 0.6840202  0.35353592]\n","  [0.34111387 0.2972543  0.41706875 ... 0.37794158 0.68390334 0.34580135]\n","  [0.3496281  0.29785657 0.41336313 ... 0.37822    0.67424953 0.34633207]\n","  [0.3376245  0.29840437 0.40891314 ... 0.37774372 0.67477894 0.35149947]]\n","\n"," [[0.3769011  0.5721493  0.4358466  ... 0.27507958 0.4288712  0.5873514 ]\n","  [0.37967443 0.57849514 0.41524684 ... 0.27226767 0.41850993 0.57074773]\n","  [0.36412197 0.5736416  0.43838695 ... 0.28644666 0.40918273 0.5637267 ]\n","  [0.38487422 0.5755711  0.41488364 ... 0.25383228 0.4138822  0.5781957 ]\n","  [0.36907834 0.58355045 0.42104796 ... 0.31046832 0.4207111  0.55634177]]\n","\n"," ...\n","\n"," [[0.49594092 0.40110385 0.38960177 ... 0.43041867 0.62023693 0.35451698]\n","  [0.5258743  0.39598563 0.4131436  ... 0.40504307 0.5975038  0.33928192]\n","  [0.52463585 0.40529236 0.41725507 ... 0.408834   0.59267926 0.35565564]\n","  [0.48913288 0.38354298 0.37694904 ... 0.41984177 0.6324648  0.3363537 ]\n","  [0.5148734  0.3981512  0.40269953 ... 0.4165413  0.60774434 0.34434313]]\n","\n"," [[0.36284676 0.5523446  0.3569915  ... 0.33110824 0.3103775  0.4863298 ]\n","  [0.35946348 0.555534   0.36467656 ... 0.33508998 0.31083027 0.48707348]\n","  [0.36624837 0.566022   0.36663198 ... 0.33636993 0.31079993 0.49006754]\n","  [0.35612243 0.54237664 0.35427362 ... 0.3275697  0.3070268  0.4975128 ]\n","  [0.35104236 0.555966   0.37665334 ... 0.34195763 0.31279653 0.48221445]]\n","\n"," [[0.6243687  0.42420262 0.42170197 ... 0.49062312 0.2998932  0.380286  ]\n","  [0.6254942  0.42530006 0.41831866 ... 0.49533117 0.2972065  0.3770508 ]\n","  [0.61910546 0.41840857 0.42778617 ... 0.51764923 0.29605043 0.39744037]\n","  [0.61837584 0.42450497 0.42426986 ... 0.5130508  0.29975194 0.3979866 ]\n","  [0.6213366  0.42248398 0.42633602 ... 0.49970928 0.30140066 0.38837588]]], shape=(64, 5, 64), dtype=float32)\n"]}]},{"cell_type":"code","source":["attention(queries, keys, values, d_k).shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Rld781t2qWbn","executionInfo":{"status":"ok","timestamp":1721392866115,"user_tz":-330,"elapsed":1607,"user":{"displayName":"Sahil Sk","userId":"12371162085645398510"}},"outputId":"45c4b3ed-04db-4c41-ac1f-b4ed3b77f132"},"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/plain":["TensorShape([64, 5, 64])"]},"metadata":{},"execution_count":15}]},{"cell_type":"code","source":["from tensorflow import math, matmul, reshape, shape, transpose, cast, float32\n","from tensorflow.keras.layers import Dense, Layer\n","from tensorflow.keras.backend import softmax\n","# Implementing the Scaled-Dot Product Attention\n","class DotProductAttention(Layer):\n","    def __init__(self, **kwargs):\n","        super().__init__(**kwargs)\n","    def call(self, queries, keys, values, d_k, mask=None):\n","        # Scoring the queries against the keys after transposing the latter, and scaling\n","        scores = matmul(queries, keys, transpose_b=True) / math.sqrt(cast(d_k, float32))\n","        # Apply mask to the attention scores\n","        if mask is not None:\n","             scores += -1e9 * mask\n","        # Computing the weights by a softmax operation\n","        weights = softmax(scores)\n","        # Computing the attention by a weighted sum of the value vectors\n","        return matmul(weights, values)\n"],"metadata":{"id":"h7qyUQZLszpU","executionInfo":{"status":"ok","timestamp":1721392866116,"user_tz":-330,"elapsed":17,"user":{"displayName":"Sahil Sk","userId":"12371162085645398510"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":[" # Implementing the Multi-Head Attention\n","class MultiHeadAttention(Layer):\n","    def __init__(self, h, d_k, d_v, d_model, **kwargs):\n","        super().__init__(**kwargs)\n","        self.attention = DotProductAttention() # Scaled dot product attention\n","        self.heads = h # Number of attention heads to use\n","        self.d_k = d_k # Dimensionality of the linearly projected queries and keys\n","        self.d_v = d_v # Dimensionality of the linearly projected values\n","        self.d_model = d_model # Dimensionality of the model\n","        self.W_q = Dense(d_k)\n","        # Learned projection matrix for the queries\n","        self.W_k = Dense(d_k)\n","        # Learned projection matrix for the keys\n","        self.W_v = Dense(d_v)\n","        # Learned projection matrix for the values\n","        self.W_o = Dense(d_model) # Learned projection matrix for the multi-head output\n","    def reshape_tensor(self, x, heads, flag):\n","        if flag:\n","            # Tensor shape after reshaping and transposing:\n","            # (batch_size, heads, seq_length, -1)\n","            x = reshape(x, shape=(shape(x)[0], shape(x)[1], heads, -1))\n","            x = transpose(x, perm=(0, 2, 1, 3))\n","        else:\n","            # Reverting the reshaping and transposing operations:\n","            # (batch_size, seq_length, d_k)\n","            x = transpose(x, perm=(0, 2, 1, 3))\n","            x = reshape(x, shape=(shape(x)[0], shape(x)[1], self.d_k))\n","        return x\n","    def call(self, queries, keys, values, mask=None):\n","        # Rearrange the queries to be able to compute all heads in parallel\n","        q_reshaped = self.reshape_tensor(self.W_q(queries), self.heads, True)\n","        # Resulting tensor shape: (batch_size, heads, input_seq_length, -1)\n","        # Rearrange the keys to be able to compute all heads in parallel\n","        k_reshaped = self.reshape_tensor(self.W_k(keys), self.heads, True)\n","        # Resulting tensor shape: (batch_size, heads, input_seq_length, -1)\n","        # Rearrange the values to be able to compute all heads in parallel\n","        v_reshaped = self.reshape_tensor(self.W_v(values), self.heads, True)\n","        # Resulting tensor shape: (batch_size, heads, input_seq_length, -1)\n","        # Compute the multi-head attention output using the reshaped queries,\n","        # keys, and values\n","        o_reshaped = self.attention(q_reshaped, k_reshaped, v_reshaped, self.d_k, mask)\n","        # Resulting tensor shape: (batch_size, heads, input_seq_length, -1)\n","        # Rearrange back the output into concatenated form\n","        output = self.reshape_tensor(o_reshaped, self.heads, False)\n","        # Resulting tensor shape: (batch_size, input_seq_length, d_v)\n","        # Apply one final linear projection to the output to generate the multi-head\n","        # attention. Resulting tensor shape: (batch_size, input_seq_length, d_model)\n","        return self.W_o(output)"],"metadata":{"id":"Idw4Uhr0tJtU","executionInfo":{"status":"ok","timestamp":1721392866116,"user_tz":-330,"elapsed":16,"user":{"displayName":"Sahil Sk","userId":"12371162085645398510"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["input_seq_length = 5 # Maximum length of the input sequence\n","h = 8 # Number of self-attention heads\n","d_k = 64 # Dimensionality of the linearly projected queries and keys\n","d_v = 64 # Dimensionality of the linearly projected values\n","d_model = 512 # Dimensionality of the model sub-layers' outputs\n","batch_size = 64 # Batch size from the training process\n","queries = random.random((batch_size, input_seq_length, d_k))\n","keys = random.random((batch_size, input_seq_length, d_k))\n","values = random.random((batch_size, input_seq_length, d_v))\n","multihead_attention = MultiHeadAttention(h, d_k, d_v, d_model)\n","print(multihead_attention(queries, keys, values))"],"metadata":{"id":"qrmJDg807Kko","executionInfo":{"status":"ok","timestamp":1721392866116,"user_tz":-330,"elapsed":16,"user":{"displayName":"Sahil Sk","userId":"12371162085645398510"}},"outputId":"29b4bbd8-cc42-4cb1-ee1b-2256ae6dc492","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["tf.Tensor(\n","[[[-2.29031779e-02  6.64517581e-02 -3.04333027e-02 ... -1.62157968e-01\n","   -2.60966092e-01  1.22116573e-01]\n","  [-2.39934009e-02  6.35017008e-02 -2.95974482e-02 ... -1.59681827e-01\n","   -2.63196468e-01  1.25780284e-01]\n","  [-2.39648297e-02  6.50642812e-02 -3.07038426e-02 ... -1.61196709e-01\n","   -2.61221588e-01  1.23951055e-01]\n","  [-2.17814632e-02  6.42109215e-02 -3.06741744e-02 ... -1.59833014e-01\n","   -2.62974173e-01  1.23221822e-01]\n","  [-2.19890848e-02  6.20770715e-02 -2.99076606e-02 ... -1.56092718e-01\n","   -2.61918038e-01  1.26674935e-01]]\n","\n"," [[ 1.03055805e-01  1.61226377e-01 -6.43157512e-02 ... -2.22388551e-01\n","   -3.20952386e-01  6.74936846e-02]\n","  [ 9.98393893e-02  1.61328539e-01 -6.43872917e-02 ... -2.23210454e-01\n","   -3.18335086e-01  7.05329478e-02]\n","  [ 1.01821452e-01  1.61877424e-01 -6.32952750e-02 ... -2.21641123e-01\n","   -3.20943594e-01  6.91150948e-02]\n","  [ 1.01731636e-01  1.61925539e-01 -6.52155131e-02 ... -2.21485630e-01\n","   -3.20492476e-01  6.72967806e-02]\n","  [ 1.02103479e-01  1.61433652e-01 -6.26895949e-02 ... -2.20869467e-01\n","   -3.20802599e-01  6.62161782e-02]]\n","\n"," [[-1.21374996e-02  1.44291192e-01 -6.93837628e-02 ... -2.02572688e-01\n","   -2.79430062e-01  4.19634655e-02]\n","  [-1.51126599e-02  1.49221390e-01 -7.23441541e-02 ... -2.03578219e-01\n","   -2.78772295e-01  4.03451174e-02]\n","  [-1.22376774e-02  1.43075615e-01 -7.28197619e-02 ... -2.05148071e-01\n","   -2.77520180e-01  3.74535285e-02]\n","  [-1.36909103e-02  1.48811638e-01 -7.64688030e-02 ... -2.03481510e-01\n","   -2.79957473e-01  4.05114554e-02]\n","  [-1.08079324e-02  1.44972146e-01 -7.56861120e-02 ... -2.02321991e-01\n","   -2.77501523e-01  3.95889543e-02]]\n","\n"," ...\n","\n"," [[-3.38485325e-03  1.49348512e-01 -2.20838096e-02 ... -1.69263080e-01\n","   -3.78032267e-01  4.47826423e-02]\n","  [ 6.52131494e-05  1.50729895e-01 -2.55533643e-02 ... -1.70600191e-01\n","   -3.79544526e-01  4.27072719e-02]\n","  [-5.57007479e-05  1.49146289e-01 -2.26811860e-02 ... -1.69246137e-01\n","   -3.77090603e-01  4.35104743e-02]\n","  [-1.48751994e-03  1.51181549e-01 -2.26328503e-02 ... -1.70657039e-01\n","   -3.78693342e-01  4.25049923e-02]\n","  [-1.43215363e-03  1.50885805e-01 -2.05691028e-02 ... -1.68462798e-01\n","   -3.77908468e-01  4.61722426e-02]]\n","\n"," [[-2.40553599e-02  3.34046334e-02 -2.64071431e-02 ... -3.39572169e-02\n","   -2.80227691e-01  1.17261365e-01]\n","  [-2.31059622e-02  4.04504463e-02 -3.20021249e-02 ... -3.70479077e-02\n","   -2.80286461e-01  1.16740167e-01]\n","  [-2.12934725e-02  4.26051058e-02 -3.31072435e-02 ... -3.85495722e-02\n","   -2.78284520e-01  1.15827970e-01]\n","  [-2.49134842e-02  3.80588137e-02 -2.89510619e-02 ... -3.75991538e-02\n","   -2.77930349e-01  1.19437590e-01]\n","  [-2.43987534e-02  3.79257835e-02 -2.87095848e-02 ... -3.77619229e-02\n","   -2.80507565e-01  1.18366085e-01]]\n","\n"," [[ 1.54436976e-01  2.33228877e-01 -1.09246656e-01 ... -2.61706561e-01\n","   -3.29720765e-01 -2.60167476e-02]\n","  [ 1.54863313e-01  2.30500028e-01 -1.04497023e-01 ... -2.65555441e-01\n","   -3.24832320e-01 -2.45622210e-02]\n","  [ 1.52935028e-01  2.28473455e-01 -1.03851728e-01 ... -2.65171140e-01\n","   -3.23842317e-01 -2.25106440e-02]\n","  [ 1.53489485e-01  2.33547717e-01 -1.04910739e-01 ... -2.62128294e-01\n","   -3.27512443e-01 -2.56217271e-02]\n","  [ 1.50286257e-01  2.31192812e-01 -1.02201290e-01 ... -2.59905368e-01\n","   -3.28104496e-01 -2.24206746e-02]]], shape=(64, 5, 512), dtype=float32)\n"]}]},{"cell_type":"code","source":["from tensorflow.keras.layers import LayerNormalization, Layer, Dense, ReLU, Dropout"],"metadata":{"id":"YYnPkzzk7L6f","executionInfo":{"status":"ok","timestamp":1721392866116,"user_tz":-330,"elapsed":12,"user":{"displayName":"Sahil Sk","userId":"12371162085645398510"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["# Implementing the Add & Norm Layer\n","class AddNormalization(Layer):\n","    def __init__(self, **kwargs):\n","        super().__init__(**kwargs)\n","        self.layer_norm = LayerNormalization()\n","        # Layer normalization layer\n","    def call(self, x, sublayer_x):\n","        # The sublayer input and output need to be of the same shape to be summed\n","        add = x + sublayer_x\n","        # Apply layer normalization to the sum\n","        return self.layer_norm(add)\n"],"metadata":{"id":"lUwnf8wqLtH2","executionInfo":{"status":"ok","timestamp":1721392866116,"user_tz":-330,"elapsed":12,"user":{"displayName":"Sahil Sk","userId":"12371162085645398510"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["# Implementing the Feed-Forward Layer\n","class FeedForward(Layer):\n","    def __init__(self, d_ff, d_model, **kwargs):\n","        super().__init__(**kwargs)\n","        self.fully_connected1 = Dense(d_ff) # First fully connected layer\n","        self.fully_connected2 = Dense(d_model) # Second fully connected layer\n","        self.activation = ReLU() # ReLU activation layer\n","    def call(self, x):\n","        # The input is passed into the two fully-connected layers, with a ReLU in between\n","        x_fc1 = self.fully_connected1(x)\n","        return self.fully_connected2(self.activation(x_fc1))"],"metadata":{"id":"tHsCCUkTMCed","executionInfo":{"status":"ok","timestamp":1721392866116,"user_tz":-330,"elapsed":11,"user":{"displayName":"Sahil Sk","userId":"12371162085645398510"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["class EncoderLayer(Layer):\n","    def __init__(self, sequence_length,h, d_k, d_v, d_model, d_ff, rate, **kwargs):\n","       super().__init__(**kwargs)\n","       self.build(input_shape=[None, sequence_length, d_model])\n","       self.multihead_attention= MultiHeadAttention(h, d_k, d_v, d_model)\n","       self.dropout1=Dropout(rate)\n","       self.add_norm1=AddNormalization()\n","       self.feed_forward=FeedForward(d_ff,d_model)\n","       self.dropout2=Dropout(rate)\n","       self.add_norm2=AddNormalization()\n","       self.sequence_length = sequence_length\n","       self.d_model=d_model\n","\n","    def build_graph(self):\n","       # this function print model summary\n","       input_layer = Input(shape=(self.sequence_length, self.d_model))\n","       return Model(inputs=[input_layer], outputs=self.call(input_layer, None, True))\n","\n","    def cell(self,x, padding_mask, training):\n","      # Multi-head attention layer\n","      multihead_output=self.multihead_attention(x,x,x,padding_mask)\n","      # Add in a dropout layer\n","      multihead_output=self.dropout1(multihead_output,training=training)\n","      # Add & Norm layer\n","      add_norm1_output=self.add_norm1(x,multihead_output)\n","\n","      # fully connected layer\n","\n","      fully_connected_output=self.feed_forward(add_norm1_output)\n","      # Add in a dropout layer\n","      fully_connected_output=self.dropout2(fully_connected_output,training=training)\n","      # Add & Norm layer\n","      add_norm2_output=self.add_norm2(add_norm1_output,fully_connected_output)\n","\n","      ## return output\n","      return add_norm2_output"],"metadata":{"id":"Nf4cietSHtUl","executionInfo":{"status":"ok","timestamp":1721392866116,"user_tz":-330,"elapsed":11,"user":{"displayName":"Sahil Sk","userId":"12371162085645398510"}}},"execution_count":22,"outputs":[]},{"cell_type":"code","source":["# Implementing the Encoder\n","class Encoder(Layer):\n","    def __init__(self, vocab_size, sequence_length, h, d_k, d_v, d_model, d_ff, n, rate,\n","    **kwargs):\n","        super().__init__(**kwargs)\n","        self.pos_encoding = PositionEmbeddingFixedWeights(sequence_length, vocab_size,\n","        d_model)\n","        self.dropout = Dropout(rate)\n","        self.encoder_layer = [EncoderLayer(sequence_length,h, d_k, d_v, d_model, d_ff, rate)\n","        for _ in range(n)]\n","    def call(self, input_sentence, padding_mask, training):\n","        # Generate the positional encoding\n","        pos_encoding_output = self.pos_encoding(input_sentence)\n","        # Expected output shape = (batch_size, sequence_length, d_model)\n","        # Add in a dropout layer\n","        x = self.dropout(pos_encoding_output, training=training)\n","        # Pass on the positional encoded values to each encoder layer\n","        for i, layer in enumerate(self.encoder_layer):\n","           x = layer(x, padding_mask, training)\n","        return x"],"metadata":{"id":"pOdmoLdTJuGt","executionInfo":{"status":"ok","timestamp":1721392866117,"user_tz":-330,"elapsed":12,"user":{"displayName":"Sahil Sk","userId":"12371162085645398510"}}},"execution_count":23,"outputs":[]},{"cell_type":"code","source":["enc_vocab_size = 20 # Vocabulary size for the encoder\n","input_seq_length = 5 # Maximum length of the input sequence\n","h = 8 # Number of self-attention heads\n","d_k = 64 # Dimensionality of the linearly projected queries and keys\n","d_v = 64 # Dimensionality of the linearly projected values\n","d_ff = 2048 # Dimensionality of the inner fully connected layer\n","d_model = 512 # Dimensionality of the model sub-layers' outputs\n","n = 6 # Number of layers in the encoder stack\n","batch_size = 64 # Batch size from the training process\n","dropout_rate = 0.1 # Frequency of dropping the input units in the dropout layers\n","input_seq = random.random((batch_size, input_seq_length))\n","encoder = Encoder(enc_vocab_size, input_seq_length, h, d_k, d_v, d_model, d_ff, n,\n","dropout_rate)\n","print(encoder(input_seq, None, True))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MPTgOSd_K21o","executionInfo":{"status":"ok","timestamp":1721392866117,"user_tz":-330,"elapsed":12,"user":{"displayName":"Sahil Sk","userId":"12371162085645398510"}},"outputId":"2e3cfd82-a2f7-4d49-acc2-51b28e16ed49"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["tf.Tensor(\n","[[[ 0.00000000e+00  2.22222233e+00  0.00000000e+00 ...  0.00000000e+00\n","    0.00000000e+00  2.22222233e+00]\n","  [ 9.34967756e-01  1.71144700e+00  9.13173616e-01 ...  2.22222233e+00\n","    1.15181436e-04  2.22222233e+00]\n","  [ 1.01033056e+00  6.48725748e-01  1.04046082e+00 ...  2.22222233e+00\n","    2.30362872e-04  2.22222233e+00]\n","  [ 1.56800017e-01  1.11194458e-02  2.72317141e-01 ...  2.22222233e+00\n","    3.45544337e-04  2.22222233e+00]\n","  [-8.40891719e-01  3.84840459e-01 -7.30185390e-01 ...  2.22222209e+00\n","    4.60725743e-04  2.22222233e+00]]\n","\n"," [[ 0.00000000e+00  2.22222233e+00  0.00000000e+00 ...  2.22222233e+00\n","    0.00000000e+00  2.22222233e+00]\n","  [ 9.34967756e-01  1.71144700e+00  9.13173616e-01 ...  2.22222233e+00\n","    1.15181436e-04  2.22222233e+00]\n","  [ 1.01033056e+00  6.48725748e-01  1.04046082e+00 ...  2.22222233e+00\n","    2.30362872e-04  2.22222233e+00]\n","  [ 1.56800017e-01  1.11194458e-02  2.72317141e-01 ...  2.22222233e+00\n","    3.45544337e-04  2.22222233e+00]\n","  [ 0.00000000e+00  3.84840459e-01 -7.30185390e-01 ...  2.22222209e+00\n","    4.60725743e-04  0.00000000e+00]]\n","\n"," [[ 0.00000000e+00  2.22222233e+00  0.00000000e+00 ...  2.22222233e+00\n","    0.00000000e+00  2.22222233e+00]\n","  [ 9.34967756e-01  1.71144700e+00  0.00000000e+00 ...  0.00000000e+00\n","    1.15181436e-04  2.22222233e+00]\n","  [ 1.01033056e+00  6.48725748e-01  1.04046082e+00 ...  2.22222233e+00\n","    2.30362872e-04  2.22222233e+00]\n","  [ 1.56800017e-01  1.11194458e-02  2.72317141e-01 ...  2.22222233e+00\n","    3.45544337e-04  2.22222233e+00]\n","  [-8.40891719e-01  3.84840459e-01 -7.30185390e-01 ...  2.22222209e+00\n","    4.60725743e-04  2.22222233e+00]]\n","\n"," ...\n","\n"," [[ 0.00000000e+00  2.22222233e+00  0.00000000e+00 ...  2.22222233e+00\n","    0.00000000e+00  2.22222233e+00]\n","  [ 9.34967756e-01  1.71144700e+00  9.13173616e-01 ...  2.22222233e+00\n","    1.15181436e-04  2.22222233e+00]\n","  [ 1.01033056e+00  6.48725748e-01  1.04046082e+00 ...  2.22222233e+00\n","    0.00000000e+00  2.22222233e+00]\n","  [ 1.56800017e-01  1.11194458e-02  2.72317141e-01 ...  2.22222233e+00\n","    0.00000000e+00  2.22222233e+00]\n","  [-8.40891719e-01  3.84840459e-01 -7.30185390e-01 ...  0.00000000e+00\n","    4.60725743e-04  2.22222233e+00]]\n","\n"," [[ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  2.22222233e+00\n","    0.00000000e+00  2.22222233e+00]\n","  [ 9.34967756e-01  1.71144700e+00  9.13173616e-01 ...  2.22222233e+00\n","    0.00000000e+00  2.22222233e+00]\n","  [ 1.01033056e+00  6.48725748e-01  1.04046082e+00 ...  2.22222233e+00\n","    0.00000000e+00  2.22222233e+00]\n","  [ 1.56800017e-01  1.11194458e-02  2.72317141e-01 ...  2.22222233e+00\n","    3.45544337e-04  2.22222233e+00]\n","  [ 0.00000000e+00  3.84840459e-01 -7.30185390e-01 ...  0.00000000e+00\n","    4.60725743e-04  2.22222233e+00]]\n","\n"," [[ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  2.22222233e+00\n","    0.00000000e+00  2.22222233e+00]\n","  [ 9.34967756e-01  1.71144700e+00  0.00000000e+00 ...  2.22222233e+00\n","    1.15181436e-04  2.22222233e+00]\n","  [ 1.01033056e+00  6.48725748e-01  1.04046082e+00 ...  2.22222233e+00\n","    2.30362872e-04  2.22222233e+00]\n","  [ 1.56800017e-01  1.11194458e-02  2.72317141e-01 ...  2.22222233e+00\n","    0.00000000e+00  2.22222233e+00]\n","  [-8.40891719e-01  3.84840459e-01 -7.30185390e-01 ...  2.22222209e+00\n","    4.60725743e-04  0.00000000e+00]]], shape=(64, 5, 512), dtype=float32)\n"]}]},{"cell_type":"code","source":["## decoder\n"],"metadata":{"id":"_Lal9q5nLBCo","executionInfo":{"status":"ok","timestamp":1721392866117,"user_tz":-330,"elapsed":9,"user":{"displayName":"Sahil Sk","userId":"12371162085645398510"}}},"execution_count":25,"outputs":[]},{"cell_type":"code","source":["class DecoderLayer(Layer):\n","    def __init__(self,sequence_length, h, d_k, d_v, d_model, d_ff, rate, **kwargs):\n","        super().__init__(**kwargs)\n","        self.build(input_shape=[None, sequence_length, d_model])\n","        self.multihead_attention1 = MultiHeadAttention(h, d_k, d_v, d_model)\n","        self.dropout1 = Dropout(rate)\n","        self.add_norm1 = AddNormalization()\n","        self.multihead_attention2 = MultiHeadAttention(h, d_k, d_v, d_model)\n","        self.dropout2 = Dropout(rate)\n","        self.add_norm2 = AddNormalization()\n","        self.feed_forward = FeedForward(d_ff, d_model)\n","        self.dropout3 = Dropout(rate)\n","        self.add_norm3 = AddNormalization()\n","        self.sequence_length = sequence_length\n","        self.d_model = d_model\n","\n","    def build_graph(self):\n","      # print decoder model summary\n","        input_layer = Input(shape=(self.sequence_length, self.d_model))\n","        return Model(inputs=[input_layer],\n","        outputs=self.call(input_layer, input_layer, None, None, True))\n","\n","    def call(self, x, encoder_output, lookahead_mask, padding_mask, training):\n","        # Multi-head attention layer\n","        multihead_output1 = self.multihead_attention1(x, x, x, lookahead_mask)\n","        # Expected output shape = (batch_size, sequence_length, d_model)\n","        # Add in a dropout layer\n","        multihead_output1 = self.dropout1(multihead_output1, training=training)\n","        # Followed by an Add & Norm layer\n","        addnorm_output1 = self.add_norm1(x, multihead_output1)\n","        # Expected output shape = (batch_size, sequence_length, d_model)\n","        # Followed by another multi-head attention layer\n","        multihead_output2 = self.multihead_attention2(addnorm_output1, encoder_output,\n","        encoder_output, padding_mask)\n","        # Add in another dropout layer\n","        multihead_output2 = self.dropout2(multihead_output2, training=training)\n","        # Followed by another Add & Norm layer\n","        addnorm_output2 = self.add_norm1(addnorm_output1, multihead_output2)\n","        # Followed by a fully connected layer\n","        feedforward_output = self.feed_forward(addnorm_output2)\n","        # Expected output shape = (batch_size, sequence_length, d_model)\n","        # Add in another dropout layer\n","        feedforward_output = self.dropout3(feedforward_output, training=training)\n","        # Followed by another Add & Norm layer\n","        return self.add_norm3(addnorm_output2, feedforward_output)"],"metadata":{"id":"E7M5CxRIcqVN","executionInfo":{"status":"ok","timestamp":1721392866117,"user_tz":-330,"elapsed":8,"user":{"displayName":"Sahil Sk","userId":"12371162085645398510"}}},"execution_count":26,"outputs":[]},{"cell_type":"code","source":["# Implementing the Decoder\n","class Decoder(Layer):\n","    def __init__(self, vocab_size, sequence_length, h, d_k, d_v, d_model, d_ff, n, rate,\n","    **kwargs):\n","        super().__init__(**kwargs)\n","        self.pos_encoding = PositionEmbeddingFixedWeights(sequence_length, vocab_size,\n","        d_model)\n","        self.dropout = Dropout(rate)\n","        self.decoder_layer = [DecoderLayer(sequence_length,h, d_k, d_v, d_model, d_ff, rate)\n","        for _ in range(n)]\n","    def call(self, output_target, encoder_output, lookahead_mask, padding_mask, training):\n","        # Generate the positional encoding\n","        pos_encoding_output = self.pos_encoding(output_target)\n","        # Expected output shape = (number of sentences, sequence_length, d_model)\n","        # Add in a dropout layer\n","        x = self.dropout(pos_encoding_output, training=training)\n","        # Pass on the positional encoded values to each encoder layer\n","        for i, layer in enumerate(self.decoder_layer):\n","            x = layer(x, encoder_output, lookahead_mask, padding_mask, training)\n","        return x"],"metadata":{"id":"bXklhMNNdAPa","executionInfo":{"status":"ok","timestamp":1721392866117,"user_tz":-330,"elapsed":8,"user":{"displayName":"Sahil Sk","userId":"12371162085645398510"}}},"execution_count":27,"outputs":[]},{"cell_type":"code","source":["dec_vocab_size = 20 # Vocabulary size for the decoder\n","input_seq_length = 5 # Maximum length of the input sequence\n","h = 8 # Number of self-attention heads\n","d_k = 64 # Dimensionality of the linearly projected queries and keys\n","d_v = 64 # Dimensionality of the linearly projected values\n","d_ff = 2048 # Dimensionality of the inner fully connected layer\n","d_model = 512 # Dimensionality of the model sub-layers' outputs\n","n = 6 # Number of layers in the decoder stack\n","batch_size = 64\n","# Batch size from the training process\n","dropout_rate = 0.1\n","# Frequency of dropping the input units in the dropout layers\n","input_seq = random.random((batch_size, input_seq_length))\n","enc_output = random.random((batch_size, input_seq_length, d_model))\n","decoder = Decoder(dec_vocab_size, input_seq_length, h, d_k, d_v, d_model, d_ff, n,\n","dropout_rate)\n","print(decoder(input_seq, enc_output, None, True))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oFRrXWUYdHaq","executionInfo":{"status":"ok","timestamp":1721392867738,"user_tz":-330,"elapsed":1629,"user":{"displayName":"Sahil Sk","userId":"12371162085645398510"}},"outputId":"fec1f5b5-18b6-4795-9844-efae5240f27b"},"execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":["tf.Tensor(\n","[[[ 0.51025665  0.79452646  0.33135727 ... -1.1658477   1.7707444\n","    0.3043663 ]\n","  [ 0.6210786   0.7615581   0.4284633  ... -1.1828421   1.766679\n","    0.31316793]\n","  [ 0.6871866   0.67443043  0.45655155 ... -1.2032787   1.7467706\n","    0.28913987]\n","  [ 0.6554414   0.6313329   0.3906897  ... -1.2289406   1.752642\n","    0.24422693]\n","  [ 0.5624084   0.6528156   0.31103334 ... -1.2239923   1.775493\n","    0.2363835 ]]\n","\n"," [[ 0.6002505   0.6596829   0.05162104 ... -0.6283706   1.3787189\n","    0.82617635]\n","  [ 0.71797377  0.6306014   0.15302037 ... -0.65532714  1.3848516\n","    0.810952  ]\n","  [ 0.78150415  0.5365808   0.18311563 ... -0.6957038   1.3752369\n","    0.7773768 ]\n","  [ 0.7480451   0.47183508  0.11130506 ... -0.7251461   1.3751017\n","    0.74939823]\n","  [ 0.6560516   0.4705173   0.01422885 ... -0.7241089   1.3933616\n","    0.7339663 ]]\n","\n"," [[ 0.524349    0.7465668   0.26670966 ... -0.9327688   1.6288153\n","    0.6124896 ]\n","  [ 0.6462786   0.7280103   0.38008657 ... -0.9600306   1.6343696\n","    0.62072647]\n","  [ 0.7157363   0.6509985   0.41074315 ... -1.0029603   1.6427696\n","    0.60352397]\n","  [ 0.6890352   0.60209227  0.33420873 ... -1.0379542   1.6443249\n","    0.5658699 ]\n","  [ 0.5890849   0.6243654   0.24342747 ... -1.0361663   1.6401178\n","    0.535688  ]]\n","\n"," ...\n","\n"," [[ 0.7449732   0.42343283  0.1966192  ... -0.43021688  1.5721055\n","    0.7096432 ]\n","  [ 0.858679    0.38597703  0.32190642 ... -0.4514425   1.5880893\n","    0.7071255 ]\n","  [ 0.9291316   0.30192932  0.37244377 ... -0.477216    1.6033044\n","    0.66206604]\n","  [ 0.90292716  0.25091046  0.31791672 ... -0.5033807   1.6124882\n","    0.6226419 ]\n","  [ 0.8173884   0.26677886  0.23100293 ... -0.5044453   1.610629\n","    0.60920566]]\n","\n"," [[ 0.9112067   0.53267384  0.37757835 ... -0.8479957   1.4044987\n","    0.54823714]\n","  [ 1.023735    0.48614618  0.48531938 ... -0.87761164  1.4218422\n","    0.569347  ]\n","  [ 1.0889841   0.40871426  0.5206058  ... -0.89852375  1.4230814\n","    0.539461  ]\n","  [ 1.0484198   0.35354665  0.46363527 ... -0.9048874   1.4223366\n","    0.49827677]\n","  [ 0.9566903   0.35484347  0.3835107  ... -0.8884051   1.4222842\n","    0.4681324 ]]\n","\n"," [[ 0.55754596  0.6001808   0.4249931  ... -0.7360234   1.4593583\n","    0.46389574]\n","  [ 0.6717361   0.5712368   0.5414957  ... -0.76247644  1.4591696\n","    0.47269773]\n","  [ 0.7265434   0.48397076  0.57097197 ... -0.7928985   1.4597993\n","    0.44840163]\n","  [ 0.7038296   0.43198207  0.49567604 ... -0.8059967   1.4610207\n","    0.411474  ]\n","  [ 0.61828977  0.43778268  0.40835235 ... -0.8053506   1.4568471\n","    0.39152664]]], shape=(64, 5, 512), dtype=float32)\n"]}]},{"cell_type":"code","source":["decoder(input_seq, enc_output, None, True).shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"exX3WfjUdSBh","executionInfo":{"status":"ok","timestamp":1721392867739,"user_tz":-330,"elapsed":8,"user":{"displayName":"Sahil Sk","userId":"12371162085645398510"}},"outputId":"9beb6d94-2087-4bbd-92dd-3f63d407fcb2"},"execution_count":29,"outputs":[{"output_type":"execute_result","data":{"text/plain":["TensorShape([64, 5, 512])"]},"metadata":{},"execution_count":29}]},{"cell_type":"code","source":["from tensorflow import math, cast, float32 ,linalg, ones,maximum, newaxis"],"metadata":{"id":"IUZ60goYdh49","executionInfo":{"status":"ok","timestamp":1721392867739,"user_tz":-330,"elapsed":5,"user":{"displayName":"Sahil Sk","userId":"12371162085645398510"}}},"execution_count":30,"outputs":[]},{"cell_type":"code","source":["# ## combain encoder and decoder model\n","class TransformerModel(Model):\n","    def __init__(self, enc_vocab_size, dec_vocab_size, enc_seq_length, dec_seq_length,\n","                 h, d_k, d_v, d_model, d_ff_inner, n, rate, **kwargs):\n","        super().__init__(**kwargs)\n","        # Set up the encoder\n","        self.encoder = Encoder(enc_vocab_size, enc_seq_length, h, d_k, d_v,\n","        d_model, d_ff_inner, n, rate)\n","        self.decoder = Decoder(dec_vocab_size, dec_seq_length, h, d_k, d_v,\n","        d_model, d_ff_inner, n, rate)\n","        # Define the final dense layer\n","        self.model_last_layer = Dense(dec_vocab_size)\n","\n","\n","    def padding_mask(self, input):\n","\n","        # Create mask which marks the zero padding values in the input by a 1.0\n","        mask = math.equal(input, 0)\n","        mask = cast(mask, float32)\n","        # The shape of the mask should be broadcastable to the shape\n","        # of the attention weights that it will be masking later on\n","        return mask[:, newaxis, newaxis, :]\n","\n","\n","    def lookahead_mask(self, shape):\n","\n","        # Mask out future entries by marking them with a 1.0\n","        mask = 1 - linalg.band_part(ones((shape, shape)), -1, 0)\n","        return mask\n","\n","\n","    def call(self, encoder_input, decoder_input, training):\n","\n","        # Create padding mask to mask the encoder inputs and the encoder\n","        # outputs in the decoder\n","        enc_padding_mask = self.padding_mask(encoder_input)\n","        # Create and combine padding and look-ahead masks to be fed into the decoder\n","        dec_in_padding_mask = self.padding_mask(decoder_input)\n","        dec_in_lookahead_mask = self.lookahead_mask(decoder_input.shape[1])\n","        dec_in_lookahead_mask = maximum(dec_in_padding_mask, dec_in_lookahead_mask)\n","        # Feed the input into the encoder\n","        encoder_output = self.encoder(encoder_input, enc_padding_mask, training)\n","        # Feed the encoder output into the decoder\n","        decoder_output = self.decoder(decoder_input, encoder_output,\n","        dec_in_lookahead_mask, enc_padding_mask, training)\n","        # Pass the decoder output through a final dense layer\n","        model_output = self.model_last_layer(decoder_output)\n","        return model_output"],"metadata":{"id":"syRmJKVlgJye","executionInfo":{"status":"ok","timestamp":1721392867739,"user_tz":-330,"elapsed":4,"user":{"displayName":"Sahil Sk","userId":"12371162085645398510"}}},"execution_count":31,"outputs":[]},{"cell_type":"code","source":["enc_vocab_size = 20 # Vocabulary size for the encoder\n","dec_vocab_size = 20 # Vocabulary size for the decoder\n","enc_seq_length = 5\n","dec_seq_length = 5\n","# Maximum length of the input sequence\n","# Maximum length of the target sequence\n","h = 8 # Number of self-attention heads\n","d_k = 64 # Dimensionality of the linearly projected queries and keys\n","d_v = 64 # Dimensionality of the linearly projected values\n","d_ff = 2048 # Dimensionality of the inner fully connected layer\n","d_model = 512 # Dimensionality of the model sub-layers' outputs\n","n = 6 # Number of layers in the encoder stack\n","dropout_rate = 0.1\n","# Frequency of dropping the input units in the dropout layers\n","\n","# Create model\n","training_model = TransformerModel(enc_vocab_size, dec_vocab_size, enc_seq_length,\n","dec_seq_length, h, d_k, d_v, d_model, d_ff, n,\n","dropout_rate)"],"metadata":{"id":"VOIvLOmchTT2","executionInfo":{"status":"ok","timestamp":1721392868531,"user_tz":-330,"elapsed":796,"user":{"displayName":"Sahil Sk","userId":"12371162085645398510"}}},"execution_count":32,"outputs":[]},{"cell_type":"code","source":["## model summary for encoder\n","\n","encoder = EncoderLayer(enc_seq_length, h, d_k, d_v, d_model, d_ff, dropout_rate)\n","encoder.build_graph().summary()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"44gdaUwLjgBY","executionInfo":{"status":"ok","timestamp":1721392868532,"user_tz":-330,"elapsed":8,"user":{"displayName":"Sahil Sk","userId":"12371162085645398510"}},"outputId":"9ea57e7e-e7c3-43c2-834f-836f5381d459"},"execution_count":33,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_1 (InputLayer)        [(None, 5, 512)]          0         \n","                                                                 \n","=================================================================\n","Total params: 0 (0.00 Byte)\n","Trainable params: 0 (0.00 Byte)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"1qMfu0bC9h-O","executionInfo":{"status":"ok","timestamp":1721392868532,"user_tz":-330,"elapsed":5,"user":{"displayName":"Sahil Sk","userId":"12371162085645398510"}}},"execution_count":33,"outputs":[]},{"cell_type":"code","source":["## decoder model summary\n","\n","decoder = DecoderLayer(dec_seq_length, h, d_k, d_v, d_model, d_ff, dropout_rate)\n","decoder.build_graph().summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Bf8An2hgmOul","executionInfo":{"status":"ok","timestamp":1721392869295,"user_tz":-330,"elapsed":768,"user":{"displayName":"Sahil Sk","userId":"12371162085645398510"}},"outputId":"4eb95585-9124-419d-aae5-8f21be2b8fca"},"execution_count":34,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model_1\"\n","__________________________________________________________________________________________________\n"," Layer (type)                Output Shape                 Param #   Connected to                  \n","==================================================================================================\n"," input_2 (InputLayer)        [(None, 5, 512)]             0         []                            \n","                                                                                                  \n"," multi_head_attention_38 (M  (None, 5, 512)               131776    ['input_2[0][0]',             \n"," ultiHeadAttention)                                                  'input_2[0][0]',             \n","                                                                     'input_2[0][0]']             \n","                                                                                                  \n"," dropout_66 (Dropout)        (None, 5, 512)               0         ['multi_head_attention_38[0][0\n","                                                                    ]']                           \n","                                                                                                  \n"," add_normalization_62 (AddN  (None, 5, 512)               1024      ['input_2[0][0]',             \n"," ormalization)                                                       'dropout_66[0][0]',          \n","                                                                     'add_normalization_62[0][0]',\n","                                                                     'dropout_67[0][0]']          \n","                                                                                                  \n"," multi_head_attention_39 (M  (None, 5, 512)               131776    ['add_normalization_62[0][0]',\n"," ultiHeadAttention)                                                  'input_2[0][0]',             \n","                                                                     'input_2[0][0]']             \n","                                                                                                  \n"," dropout_67 (Dropout)        (None, 5, 512)               0         ['multi_head_attention_39[0][0\n","                                                                    ]']                           \n","                                                                                                  \n"," feed_forward_25 (FeedForwa  (None, 5, 512)               2099712   ['add_normalization_62[1][0]']\n"," rd)                                                                                              \n","                                                                                                  \n"," dropout_68 (Dropout)        (None, 5, 512)               0         ['feed_forward_25[0][0]']     \n","                                                                                                  \n"," add_normalization_64 (AddN  (None, 5, 512)               1024      ['add_normalization_62[1][0]',\n"," ormalization)                                                       'dropout_68[0][0]']          \n","                                                                                                  \n","==================================================================================================\n","Total params: 2365312 (9.02 MB)\n","Trainable params: 2365312 (9.02 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","__________________________________________________________________________________________________\n"]}]},{"cell_type":"code","source":["## train our model\n","\n","## data pre processesing\n","from pickle import load\n","from numpy.random import shuffle\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow import convert_to_tensor, int64\n"],"metadata":{"id":"Fsj47AVd9kt9","executionInfo":{"status":"ok","timestamp":1721392869296,"user_tz":-330,"elapsed":4,"user":{"displayName":"Sahil Sk","userId":"12371162085645398510"}}},"execution_count":35,"outputs":[]},{"cell_type":"code","source":["class PrepareDataset:\n","    def __init__(self, **kwargs):\n","        super().__init__(**kwargs)\n","        self.n_sentences = 10000 # Number of sentences to include in the dataset\n","        self.train_split = 0.9 # Ratio of the training data split\n","    # Fit a tokenizer\n","    def create_tokenizer(self, dataset):\n","        tokenizer = Tokenizer()\n","        tokenizer.fit_on_texts(dataset)\n","        return tokenizer\n","\n","    # Find the maximum length of the sequences\n","    def find_seq_length(self, dataset):\n","          return max(len(seq.split()) for seq in dataset)\n","\n","    # Find the vocabulary size\n","    def find_vocab_size(self, tokenizer, dataset):\n","        tokenizer.fit_on_texts(dataset)\n","        return len(tokenizer.word_index) + 1\n","\n","\n","    def __call__(self, filename, **kwargs):\n","        # Load a clean dataset\n","        clean_dataset = load(open(filename, 'rb'))\n","        # Reduce dataset size\n","        dataset = clean_dataset[:self.n_sentences, :]\n","        # Include start and end of string tokens\n","        for i in range(dataset[:, 0].size):\n","            dataset[i, 0] = \"<START> \" + dataset[i, 0] + \" <EOS>\"\n","            dataset[i, 1] = \"<START> \" + dataset[i, 1] + \" <EOS>\"\n","        # Random shuffle the dataset\n","        shuffle(dataset)\n","        # Split the dataset\n","        train = dataset[:int(self.n_sentences * self.train_split)]\n","        # Prepare tokenizer for the encoder input\n","        enc_tokenizer = self.create_tokenizer(train[:, 0])\n","        enc_seq_length = self.find_seq_length(train[:, 0])\n","        enc_vocab_size = self.find_vocab_size(enc_tokenizer, train[:, 0])\n","        # Encode and pad the input sequences\n","        trainX = enc_tokenizer.texts_to_sequences(train[:, 0])\n","        trainX = pad_sequences(trainX, maxlen=enc_seq_length, padding='post')\n","        trainX = convert_to_tensor(trainX, dtype=int64)\n","        # Prepare tokenizer for the decoder input\n","        dec_tokenizer = self.create_tokenizer(train[:, 1])\n","        dec_seq_length = self.find_seq_length(train[:, 1])\n","        dec_vocab_size = self.find_vocab_size(dec_tokenizer, train[:, 1])\n","        # Encode and pad the input sequences\n","        trainY = dec_tokenizer.texts_to_sequences(train[:, 1])\n","        trainY = pad_sequences(trainY, maxlen=dec_seq_length, padding='post')\n","        trainY = convert_to_tensor(trainY, dtype=int64)\n","        return (trainX, trainY, train, enc_seq_length, dec_seq_length,\n","        enc_vocab_size, dec_vocab_size)"],"metadata":{"id":"57gbQORi91Ge","executionInfo":{"status":"ok","timestamp":1721392869296,"user_tz":-330,"elapsed":4,"user":{"displayName":"Sahil Sk","userId":"12371162085645398510"}}},"execution_count":36,"outputs":[]},{"cell_type":"code","source":["# Prepare the training data\n","dataset = PrepareDataset()\n","trainX, trainY, train_orig, enc_seq_length, dec_seq_length, \\\n","enc_vocab_size, dec_vocab_size = dataset('drive/MyDrive/eng_ger.pkl')"],"metadata":{"id":"_lDCjuol-ifN","executionInfo":{"status":"ok","timestamp":1721392906056,"user_tz":-330,"elapsed":1850,"user":{"displayName":"Sahil Sk","userId":"12371162085645398510"}}},"execution_count":38,"outputs":[]},{"cell_type":"code","source":["print(train_orig[0, 0], '\\n', trainX[0, :])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"l_vAirfgAzSH","executionInfo":{"status":"ok","timestamp":1721392915025,"user_tz":-330,"elapsed":905,"user":{"displayName":"Sahil Sk","userId":"12371162085645398510"}},"outputId":"d15a0205-0ee2-4e49-eca2-5edc9eac7b83"},"execution_count":39,"outputs":[{"output_type":"stream","name":"stdout","text":["<START> theyre with me <EOS> \n"," tf.Tensor([  1 146 135  11   2   0   0], shape=(7,), dtype=int64)\n"]}]},{"cell_type":"code","source":["trainX.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oEb0ZA1xA1tu","executionInfo":{"status":"ok","timestamp":1721392936170,"user_tz":-330,"elapsed":631,"user":{"displayName":"Sahil Sk","userId":"12371162085645398510"}},"outputId":"03e16059-6d6c-4229-879b-4aba042602ed"},"execution_count":40,"outputs":[{"output_type":"execute_result","data":{"text/plain":["TensorShape([9000, 7])"]},"metadata":{},"execution_count":40}]},{"cell_type":"code","source":["trainY.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MjqqDCyYA64J","executionInfo":{"status":"ok","timestamp":1721392955000,"user_tz":-330,"elapsed":819,"user":{"displayName":"Sahil Sk","userId":"12371162085645398510"}},"outputId":"2e1542d5-5dd2-48b2-f32e-bfd5c1d72257"},"execution_count":41,"outputs":[{"output_type":"execute_result","data":{"text/plain":["TensorShape([9000, 11])"]},"metadata":{},"execution_count":41}]},{"cell_type":"code","source":["enc_seq_length"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0gbxhlP1A_ZR","executionInfo":{"status":"ok","timestamp":1721392977125,"user_tz":-330,"elapsed":4,"user":{"displayName":"Sahil Sk","userId":"12371162085645398510"}},"outputId":"eb1d0f02-28b0-498a-e69f-7e50c1c31147"},"execution_count":42,"outputs":[{"output_type":"execute_result","data":{"text/plain":["7"]},"metadata":{},"execution_count":42}]},{"cell_type":"code","source":["dec_seq_length"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"V1xrS5eyBEzh","executionInfo":{"status":"ok","timestamp":1721392983892,"user_tz":-330,"elapsed":4,"user":{"displayName":"Sahil Sk","userId":"12371162085645398510"}},"outputId":"ff283000-70c6-462e-d779-a5f2ad7934ec"},"execution_count":43,"outputs":[{"output_type":"execute_result","data":{"text/plain":["11"]},"metadata":{},"execution_count":43}]},{"cell_type":"code","source":["enc_vocab_size"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vLhpHoh1BGqa","executionInfo":{"status":"ok","timestamp":1721392991868,"user_tz":-330,"elapsed":824,"user":{"displayName":"Sahil Sk","userId":"12371162085645398510"}},"outputId":"13997fde-b9d5-4c75-d6ac-d78ed76c2511"},"execution_count":44,"outputs":[{"output_type":"execute_result","data":{"text/plain":["2296"]},"metadata":{},"execution_count":44}]},{"cell_type":"code","source":["dec_vocab_size"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aQzEdAXJBIdS","executionInfo":{"status":"ok","timestamp":1721393006225,"user_tz":-330,"elapsed":621,"user":{"displayName":"Sahil Sk","userId":"12371162085645398510"}},"outputId":"0f061421-4dd7-4a79-bb27-acbd0436ab57"},"execution_count":45,"outputs":[{"output_type":"execute_result","data":{"text/plain":["3654"]},"metadata":{},"execution_count":45}]},{"cell_type":"code","source":["#traning\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.optimizers.schedules import LearningRateSchedule\n","from tensorflow.keras.metrics import Mean\n","from tensorflow import data, train, math, reduce_sum, cast, equal, argmax, \\\n","float32, GradientTape, TensorSpec, function, int64\n","from tensorflow.keras.losses import sparse_categorical_crossentropy\n","\n","from time import time"],"metadata":{"id":"paC_Vd0JBMC6","executionInfo":{"status":"ok","timestamp":1721393195617,"user_tz":-330,"elapsed":790,"user":{"displayName":"Sahil Sk","userId":"12371162085645398510"}}},"execution_count":49,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7oNtRaLWBezW","executionInfo":{"status":"ok","timestamp":1721393121308,"user_tz":-330,"elapsed":1810,"user":{"displayName":"Sahil Sk","userId":"12371162085645398510"}},"outputId":"ce60d999-08d1-4af1-fa4d-6ed54c76d403"},"execution_count":47,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[31mERROR: Could not find a version that satisfies the requirement model (from versions: none)\u001b[0m\u001b[31m\n","\u001b[0m\u001b[31mERROR: No matching distribution found for model\u001b[0m\u001b[31m\n","\u001b[0m"]}]},{"cell_type":"code","source":["# Define the model parameters\n","h = 8 # Number of self-attention heads\n","d_k = 64 # Dimensionality of the linearly projected queries and keys\n","d_v = 64 # Dimensionality of the linearly projected values\n","d_model = 512 # Dimensionality of model layers' outputs\n","d_ff = 2048 # Dimensionality of the inner fully connected layer\n","n = 6 # Number of layers in the encoder stack\n","# Define the training parameters\n","epochs = 5\n","batch_size = 64\n","beta_1 = 0.9\n","beta_2 = 0.98\n","epsilon = 1e-9\n","dropout_rate = 0.1\n","# Implementing a learning rate scheduler\n","class LRScheduler(LearningRateSchedule):\n","    def __init__(self, d_model, warmup_steps=4000, **kwargs):\n","        super().__init__(**kwargs)\n","        self.d_model = cast(d_model, float32)\n","        self.warmup_steps = warmup_steps\n","    def __call__(self, step_num):\n","        # Linearly increasing the learning rate for the first warmup_steps, and\n","        # decreasing it thereafter\n","        step_num = cast(step_num, float32)\n","        arg1 = step_num ** -0.5\n","        arg2 = step_num * (self.warmup_steps ** -1.5)\n","        return (self.d_model ** -0.5) * math.minimum(arg1, arg2)"],"metadata":{"id":"9j8x5sFTBn0W","executionInfo":{"status":"ok","timestamp":1721394542685,"user_tz":-330,"elapsed":719,"user":{"displayName":"Sahil Sk","userId":"12371162085645398510"}}},"execution_count":56,"outputs":[]},{"cell_type":"code","source":["optimizer = Adam(LRScheduler(d_model), beta_1, beta_2, epsilon)\n","# Prepare the training and test splits of the dataset\n","dataset = PrepareDataset()\n","trainX, trainY, train_orig, enc_seq_length, dec_seq_length, \\\n","enc_vocab_size, dec_vocab_size = dataset('drive/MyDrive/eng_ger.pkl')\n","# Prepare the dataset batches\n","train_dataset = data.Dataset.from_tensor_slices((trainX, trainY))\n","train_dataset = train_dataset.batch(batch_size)\n","# Create model\n","training_model = TransformerModel(enc_vocab_size, dec_vocab_size, enc_seq_length,\n","dec_seq_length, h, d_k, d_v, d_model, d_ff, n,\n","dropout_rate)\n","\n","# Defining the loss function\n","def loss_fcn(target, prediction):\n","    # Create mask so that the zero padding values are not included in the\n","    # computation of loss\n","    mask = math.logical_not(equal(target, 0))\n","    mask = cast(mask, float32)\n","    # Compute a sparse categorical cross-entropy loss on the unmasked values\n","    loss = sparse_categorical_crossentropy(target, prediction, from_logits=True) * mask\n","    # Compute the mean loss over the unmasked values\n","    return reduce_sum(loss) / reduce_sum(mask)\n","\n","# Defining the accuracy function\n","def accuracy_fcn(target, prediction):\n","    # Create mask so that the zero padding values are not included in the\n","    # computation of accuracy\n","    mask = math.logical_not(equal(target, 0))\n","\n","    # Find equal prediction and target values, and apply the padding mask\n","    accuracy = equal(target, argmax(prediction, axis=2))\n","\n","    accuracy = math.logical_and(mask, accuracy)\n","\n","    # Cast the True/False values to 32-bit-precision floating-point numbers\n","    mask = cast(mask, float32)\n","\n","    accuracy = cast(accuracy, float32)\n","\n","    # Compute the mean accuracy over the unmasked values\n","    return reduce_sum(accuracy) / reduce_sum(mask)\n","\n","# Include metrics monitoring\n","train_loss = Mean(name='train_loss')\n","train_accuracy = Mean(name='train_accuracy')\n","# Create a checkpoint object and manager to manage multiple checkpoints\n","ckpt = train.Checkpoint(model=training_model, optimizer=optimizer)\n","ckpt_manager = train.CheckpointManager(ckpt, \"./checkpoints\", max_to_keep=3)"],"metadata":{"id":"oo_BMQ2hCOTr","executionInfo":{"status":"ok","timestamp":1721394558372,"user_tz":-330,"elapsed":11281,"user":{"displayName":"Sahil Sk","userId":"12371162085645398510"}}},"execution_count":57,"outputs":[]},{"cell_type":"code","source":["# Speeding up the training process\n","@function\n","def train_step(encoder_input, decoder_input, decoder_output):\n","    with GradientTape() as tape:\n","\n","        # Run the forward pass of the model to generate a prediction\n","        prediction = training_model(encoder_input, decoder_input, training=True)\n","\n","        # Compute the training loss\n","        loss = loss_fcn(decoder_output, prediction)\n","\n","        # Compute the training accuracy\n","        accuracy = accuracy_fcn(decoder_output, prediction)\n","\n","    # Retrieve gradients of the trainable variables with respect to the training loss\n","    gradients = tape.gradient(loss, training_model.trainable_weights)\n","\n","    # Update the values of the trainable variables by gradient descent\n","    optimizer.apply_gradients(zip(gradients, training_model.trainable_weights))\n","    train_loss(loss)\n","    train_accuracy(accuracy)\n","\n","\n","for epoch in range(epochs):\n","    train_loss.reset_states()\n","\n","    train_accuracy.reset_states()\n","\n","    print(\"\\nStart of epoch %d\" % (epoch + 1))\n","\n","    start_time = time()\n","\n","    # Iterate over the dataset batches\n","    for step, (train_batchX, train_batchY) in enumerate(train_dataset):\n","    # Define the encoder and decoder inputs, and the decoder output\n","        encoder_input = train_batchX[:, 1:]\n","\n","        decoder_input = train_batchY[:, :-1]\n","\n","        decoder_output = train_batchY[:, 1:]\n","        train_step(encoder_input, decoder_input, decoder_output)\n","\n","        if step % 50 == 0:\n","            print(f\"Epoch {epoch+1} Step {step} Loss {train_loss.result():.4f} \"\n","            + f\"Accuracy {train_accuracy.result():.4f}\")\n","\n","    # Print epoch number and loss value at the end of every epoch\n","    print(f\"Epoch {epoch+1}: Training Loss {train_loss.result():.4f}, \"\n","    + f\"Training Accuracy {train_accuracy.result():.4f}\")\n","\n","    # Save a checkpoint after every five epochs\n","    if (epoch + 1) % 5 == 0:\n","        save_path = ckpt_manager.save()\n","        print(f\"Saved checkpoint at epoch {epoch+1}\")\n","\n","print(\"Total time taken: %.2fs\" % (time() - start_time))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SXmBNG2vC2F4","executionInfo":{"status":"ok","timestamp":1721395876500,"user_tz":-330,"elapsed":1308928,"user":{"displayName":"Sahil Sk","userId":"12371162085645398510"}},"outputId":"97fae40f-47b9-4ccc-d6d5-0edcb29df088"},"execution_count":58,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Start of epoch 1\n","Epoch 1 Step 0 Loss 8.3762 Accuracy 0.0000\n","Epoch 1 Step 50 Loss 7.9002 Accuracy 0.0982\n","Epoch 1 Step 100 Loss 7.2666 Accuracy 0.1585\n","Epoch 1: Training Loss 6.9172, Training Accuracy 0.1792\n","\n","Start of epoch 2\n","Epoch 2 Step 0 Loss 5.8459 Accuracy 0.2584\n","Epoch 2 Step 50 Loss 5.4896 Accuracy 0.2691\n","Epoch 2 Step 100 Loss 5.3116 Accuracy 0.2787\n","Epoch 2: Training Loss 5.1874, Training Accuracy 0.2850\n","\n","Start of epoch 3\n","Epoch 3 Step 0 Loss 4.8855 Accuracy 0.2819\n","Epoch 3 Step 50 Loss 4.6786 Accuracy 0.3119\n","Epoch 3 Step 100 Loss 4.5753 Accuracy 0.3258\n","Epoch 3: Training Loss 4.5005, Training Accuracy 0.3353\n","\n","Start of epoch 4\n","Epoch 4 Step 0 Loss 4.4162 Accuracy 0.3423\n","Epoch 4 Step 50 Loss 4.1786 Accuracy 0.3734\n","Epoch 4 Step 100 Loss 4.1084 Accuracy 0.3803\n","Epoch 4: Training Loss 4.0497, Training Accuracy 0.3864\n","\n","Start of epoch 5\n","Epoch 5 Step 0 Loss 4.0175 Accuracy 0.3926\n","Epoch 5 Step 50 Loss 3.8000 Accuracy 0.4146\n","Epoch 5 Step 100 Loss 3.7472 Accuracy 0.4201\n","Epoch 5: Training Loss 3.6939, Training Accuracy 0.4259\n","Saved checkpoint at epoch 5\n","Total time taken: 258.27s\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"Zgz7tOdrHJWS","executionInfo":{"status":"ok","timestamp":1721395989603,"user_tz":-330,"elapsed":664,"user":{"displayName":"Sahil Sk","userId":"12371162085645398510"}}},"execution_count":62,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"_GCR3P3MLDyj","executionInfo":{"status":"ok","timestamp":1721396368912,"user_tz":-330,"elapsed":819,"user":{"displayName":"Sahil Sk","userId":"12371162085645398510"}}},"execution_count":67,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Vx0bKfxAMn-A","executionInfo":{"status":"ok","timestamp":1721396583947,"user_tz":-330,"elapsed":816,"user":{"displayName":"Sahil Sk","userId":"12371162085645398510"}}},"execution_count":68,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"3yH7ELF7N3w5"},"execution_count":null,"outputs":[]}]}